{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d834ee9-7c42-42e8-bdfb-6e2ac8878e0f",
   "metadata": {
    "papermill": {
     "duration": 0.039768,
     "end_time": "2025-08-14T01:56:28.255692",
     "exception": false,
     "start_time": "2025-08-14T01:56:28.215924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee04907-74c0-4992-b5e1-dd6bea2f3b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:28.328745Z",
     "iopub.status.busy": "2025-08-14T01:56:28.327663Z",
     "iopub.status.idle": "2025-08-14T01:56:30.139279Z",
     "shell.execute_reply": "2025-08-14T01:56:30.138769Z"
    },
    "papermill": {
     "duration": 1.854655,
     "end_time": "2025-08-14T01:56:30.144295",
     "exception": false,
     "start_time": "2025-08-14T01:56:28.289640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL ของ Google Sheet\n",
    "sheet_url = \"\"\n",
    "\n",
    "# แก้ไข URL เพื่อให้ export เป็นไฟล์ CSV\n",
    "csv_export_url = sheet_url.replace(\"/edit?usp=sharing\", \"/export?format=csv\")\n",
    "\n",
    "# 3. ใช้ pandas อ่านไฟล์ CSV จาก URL ที่แก้ไขแล้ว\n",
    "try:\n",
    "    df = pd.read_csv(csv_export_url)\n",
    "    print(\"ดึงข้อมูลสำเร็จ\")\n",
    "    # แสดง 5 แถวแรกของข้อมูล\n",
    "    print(df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"เกิดข้อผิดพลาด: {e}\")\n",
    "    print(\"โปรดตรวจสอบว่า URL ถูกต้อง และชีตได้ถูกแชร์เป็นสาธารณะแล้ว\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d97f19-3acf-4b58-978c-9bbfb2183ebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:30.214290Z",
     "iopub.status.busy": "2025-08-14T01:56:30.214290Z",
     "iopub.status.idle": "2025-08-14T01:56:30.222482Z",
     "shell.execute_reply": "2025-08-14T01:56:30.220461Z"
    },
    "papermill": {
     "duration": 0.049203,
     "end_time": "2025-08-14T01:56:30.226498",
     "exception": false,
     "start_time": "2025-08-14T01:56:30.177295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b634d3-92dd-40ea-9540-50762cad8512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:30.293922Z",
     "iopub.status.busy": "2025-08-14T01:56:30.292915Z",
     "iopub.status.idle": "2025-08-14T01:56:31.511069Z",
     "shell.execute_reply": "2025-08-14T01:56:31.511069Z"
    },
    "papermill": {
     "duration": 1.257165,
     "end_time": "2025-08-14T01:56:31.516090",
     "exception": false,
     "start_time": "2025-08-14T01:56:30.258925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e753acb-e178-4c6b-8708-dbd222e479f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:31.596030Z",
     "iopub.status.busy": "2025-08-14T01:56:31.596030Z",
     "iopub.status.idle": "2025-08-14T01:56:31.602041Z",
     "shell.execute_reply": "2025-08-14T01:56:31.602041Z"
    },
    "papermill": {
     "duration": 0.055567,
     "end_time": "2025-08-14T01:56:31.608607",
     "exception": false,
     "start_time": "2025-08-14T01:56:31.553040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ดูจำนวน row ที่มี NaN ในแต่ละคอลัมน์\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# ดูจำนวน row ทั้งหมดใน DataFrame ที่มีอย่างน้อยหนึ่งค่า NaN\n",
    "print(\"Total rows with NaN:\", df.isnull().any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea975b91-a62b-4e30-b414-c2d0585090d8",
   "metadata": {
    "papermill": {
     "duration": 0.033111,
     "end_time": "2025-08-14T01:56:31.676271",
     "exception": false,
     "start_time": "2025-08-14T01:56:31.643160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61047cf-e4f8-4406-a202-750ed6f73953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:31.753756Z",
     "iopub.status.busy": "2025-08-14T01:56:31.753756Z",
     "iopub.status.idle": "2025-08-14T01:56:31.765189Z",
     "shell.execute_reply": "2025-08-14T01:56:31.763174Z"
    },
    "papermill": {
     "duration": 0.057033,
     "end_time": "2025-08-14T01:56:31.770195",
     "exception": false,
     "start_time": "2025-08-14T01:56:31.713162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df_cleaned = df.dropna()\n",
    "df = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929529d-c926-41bb-8ce9-4d2fbcc637b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:31.840903Z",
     "iopub.status.busy": "2025-08-14T01:56:31.840903Z",
     "iopub.status.idle": "2025-08-14T01:56:31.855157Z",
     "shell.execute_reply": "2025-08-14T01:56:31.855157Z"
    },
    "papermill": {
     "duration": 0.055982,
     "end_time": "2025-08-14T01:56:31.861174",
     "exception": false,
     "start_time": "2025-08-14T01:56:31.805192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9563e8-eb20-4e3e-b1ed-0a4c64d87b27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:31.937196Z",
     "iopub.status.busy": "2025-08-14T01:56:31.937196Z",
     "iopub.status.idle": "2025-08-14T01:56:31.955031Z",
     "shell.execute_reply": "2025-08-14T01:56:31.955031Z"
    },
    "papermill": {
     "duration": 0.062877,
     "end_time": "2025-08-14T01:56:31.961054",
     "exception": false,
     "start_time": "2025-08-14T01:56:31.898177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d0ae6-2230-4cf4-9527-86eef1fe75ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:32.043800Z",
     "iopub.status.busy": "2025-08-14T01:56:32.043800Z",
     "iopub.status.idle": "2025-08-14T01:56:32.050444Z",
     "shell.execute_reply": "2025-08-14T01:56:32.050444Z"
    },
    "papermill": {
     "duration": 0.052461,
     "end_time": "2025-08-14T01:56:32.055514",
     "exception": false,
     "start_time": "2025-08-14T01:56:32.003053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let pandas infer the format for each date\n",
    "df['new_datetime'] = pd.to_datetime(df['date'], format='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbeeeb2-4d81-4f0f-8d95-219bda2c3b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:32.133619Z",
     "iopub.status.busy": "2025-08-14T01:56:32.132542Z",
     "iopub.status.idle": "2025-08-14T01:56:32.139952Z",
     "shell.execute_reply": "2025-08-14T01:56:32.139952Z"
    },
    "papermill": {
     "duration": 0.053202,
     "end_time": "2025-08-14T01:56:32.145662",
     "exception": false,
     "start_time": "2025-08-14T01:56:32.092460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "df = df.drop(columns=['local_event', 'other_event']) #'local_event', 'other_event' เป็นคอลัมน์ที่ถูกเพิ่มมาทีหลังได้ไม่นาน อาจจะทำให้ประสิทธิภาพการวิจัยลดลง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd74b7-6b09-4f31-b19e-61da973d1c5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:32.222656Z",
     "iopub.status.busy": "2025-08-14T01:56:32.222656Z",
     "iopub.status.idle": "2025-08-14T01:56:32.240323Z",
     "shell.execute_reply": "2025-08-14T01:56:32.240323Z"
    },
    "papermill": {
     "duration": 0.064427,
     "end_time": "2025-08-14T01:56:32.246003",
     "exception": false,
     "start_time": "2025-08-14T01:56:32.181576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove commas from numeric strings\n",
    "for col in df.columns:\n",
    "   if df[col].dtype == 'object':\n",
    "         df[col] = df[col].astype(str).str.replace(',', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6001541f-be29-45e3-ba8c-8f27fbfed9dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:32.324033Z",
     "iopub.status.busy": "2025-08-14T01:56:32.324033Z",
     "iopub.status.idle": "2025-08-14T01:56:32.346215Z",
     "shell.execute_reply": "2025-08-14T01:56:32.346215Z"
    },
    "papermill": {
     "duration": 0.067332,
     "end_time": "2025-08-14T01:56:32.350285",
     "exception": false,
     "start_time": "2025-08-14T01:56:32.282953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert columns to appropriate data types\n",
    "df['day'] = df['day'].astype('object')\n",
    "df['date'] = pd.to_datetime(df['date'], format='mixed')\n",
    "df['cost_change'] = pd.to_numeric(df['cost_change'], errors='coerce').astype('float64')\n",
    "df['cost_material'] = pd.to_numeric(df['cost_material'], errors='coerce').astype('float64')\n",
    "df['cost_cash'] = pd.to_numeric(df['cost_cash'], errors='coerce').astype('float64')\n",
    "df['cost_banking'] = pd.to_numeric(df['cost_banking'], errors='coerce').astype('float64')\n",
    "df['cost_sum'] = pd.to_numeric(df['cost_sum'], errors='coerce').astype('float64')\n",
    "df['sales_fs'] = pd.to_numeric(df['sales_fs'], errors='coerce').astype('float64')\n",
    "df['sales_qr'] = pd.to_numeric(df['sales_qr'], errors='coerce').astype('float64')\n",
    "df['sales_gb'] = pd.to_numeric(df['sales_gb'], errors='coerce').astype('float64')\n",
    "df['sales_sp'] = pd.to_numeric(df['sales_sp'], errors='coerce').astype('float64')\n",
    "df['sales_lm'] = pd.to_numeric(df['sales_lm'], errors='coerce').astype('float64')\n",
    "df['sales_rb'] = pd.to_numeric(df['sales_rb'], errors='coerce').astype('float64')\n",
    "df['sales_sum'] = pd.to_numeric(df['sales_sum'], errors='coerce').astype('float64')\n",
    "df['profit'] = pd.to_numeric(df['profit'], errors='coerce').astype('float64')\n",
    "df['promotion_01'] = df['promotion_01'].astype('object')\n",
    "df['promotion_02'] = df['promotion_02'].astype('object')\n",
    "df['ismarketday'] = pd.to_numeric(df['ismarketday'], errors='coerce').fillna(0).astype('int64')\n",
    "df['isschoolday'] = pd.to_numeric(df['isschoolday'], errors='coerce').fillna(0).astype('int64')\n",
    "df['holiday'] = pd.to_numeric(df['holiday'], errors='coerce').fillna(0).astype('int64')\n",
    "df['isbuddaday'] = pd.to_numeric(df['isbuddaday'], errors='coerce').fillna(0).astype('int64')\n",
    "df['nd_lottery'] = pd.to_numeric(df['nd_lottery'], errors='coerce').fillna(0).astype('int64')\n",
    "df['applylkcolor'] = df['applylkcolor'].astype('object')\n",
    "df['PP'] = pd.to_numeric(df['PP'], errors='coerce').astype('float64')\n",
    "df['T'] = pd.to_numeric(df['T'], errors='coerce').astype('float64')\n",
    "df['H'] = pd.to_numeric(df['H'], errors='coerce').astype('float64')\n",
    "df['V'] = pd.to_numeric(df['V'], errors='coerce').astype('float64')\n",
    "df['pm2.5'] = pd.to_numeric(df['pm2.5'], errors='coerce').astype('float64')\n",
    "df['pm10'] = pd.to_numeric(df['pm10'], errors='coerce').astype('float64')\n",
    "df['o3'] = pd.to_numeric(df['o3'], errors='coerce').astype('float64')\n",
    "df['no2'] = pd.to_numeric(df['no2'], errors='coerce').astype('float64')\n",
    "df['pos_cash'] = pd.to_numeric(df['pos_cash'], errors='coerce').astype('float64')\n",
    "df['pos_qr'] = pd.to_numeric(df['pos_qr'], errors='coerce').astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96feb1b9-96e0-4a37-bf2b-5b7262681601",
   "metadata": {
    "papermill": {
     "duration": 0.039019,
     "end_time": "2025-08-14T01:56:32.431310",
     "exception": false,
     "start_time": "2025-08-14T01:56:32.392291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef828552-bfb4-48fb-9e82-6f02363106e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:32.511951Z",
     "iopub.status.busy": "2025-08-14T01:56:32.511951Z",
     "iopub.status.idle": "2025-08-14T01:56:32.524135Z",
     "shell.execute_reply": "2025-08-14T01:56:32.523113Z"
    },
    "papermill": {
     "duration": 0.061252,
     "end_time": "2025-08-14T01:56:32.531150",
     "exception": false,
     "start_time": "2025-08-14T01:56:32.469898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge banking cost into material cost\n",
    "df['cost_material'] = df['cost_material'] + df['cost_banking']\n",
    "df = df.drop(columns=['cost_banking'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d4ca8-36ac-4f0f-ab00-24e6413feea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:32.604860Z",
     "iopub.status.busy": "2025-08-14T01:56:32.604860Z",
     "iopub.status.idle": "2025-08-14T01:56:32.609533Z",
     "shell.execute_reply": "2025-08-14T01:56:32.609533Z"
    },
    "papermill": {
     "duration": 0.045789,
     "end_time": "2025-08-14T01:56:32.612645",
     "exception": false,
     "start_time": "2025-08-14T01:56:32.566856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge QR sales into FS sales\n",
    "df['sales_qr'] = df['sales_qr'].fillna(0)\n",
    "df['sales_fs'] = df['sales_fs'].fillna(0)\n",
    "df['sales_fs'] = df['sales_qr'] + df['sales_fs']\n",
    "df = df.drop(columns=['sales_qr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb151f7-4a8e-494f-b41b-9056b7bd3916",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:32.691405Z",
     "iopub.status.busy": "2025-08-14T01:56:32.691405Z",
     "iopub.status.idle": "2025-08-14T01:56:32.699811Z",
     "shell.execute_reply": "2025-08-14T01:56:32.699298Z"
    },
    "papermill": {
     "duration": 0.05047,
     "end_time": "2025-08-14T01:56:32.703828",
     "exception": false,
     "start_time": "2025-08-14T01:56:32.653358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map Thai day abbreviations to English\n",
    "day_mapping = {\n",
    "            'อา.': 'Sun',\n",
    "            'จ.': 'Mon',\n",
    "            'อ.': 'Tue',\n",
    "            'พ.': 'Wed',\n",
    "            'พฤ.': 'Thu',\n",
    "            'ศ.': 'Fri',\n",
    "            'ส.': 'Sat'\n",
    "        }\n",
    "df['day'] = df['day'].replace(day_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36cd25-ec68-48bd-9c05-de3760bc5a16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:32.784589Z",
     "iopub.status.busy": "2025-08-14T01:56:32.783671Z",
     "iopub.status.idle": "2025-08-14T01:56:32.800998Z",
     "shell.execute_reply": "2025-08-14T01:56:32.797975Z"
    },
    "papermill": {
     "duration": 0.065035,
     "end_time": "2025-08-14T01:56:32.807041",
     "exception": false,
     "start_time": "2025-08-14T01:56:32.742006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[df['sales_sum'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67c114-2e77-4200-82db-0f130123d6a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:32.890715Z",
     "iopub.status.busy": "2025-08-14T01:56:32.889662Z",
     "iopub.status.idle": "2025-08-14T01:56:32.909770Z",
     "shell.execute_reply": "2025-08-14T01:56:32.906645Z"
    },
    "papermill": {
     "duration": 0.076659,
     "end_time": "2025-08-14T01:56:32.918777",
     "exception": false,
     "start_time": "2025-08-14T01:56:32.842118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adjust Friday sales by adding 6000\n",
    "df.loc[df['day'] == 'Fri', 'sales_sum'] = df.loc[df['day'] == 'Fri', 'sales_sum'] + 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f0249-0c10-44f3-b530-813f42792869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:33.022349Z",
     "iopub.status.busy": "2025-08-14T01:56:33.021347Z",
     "iopub.status.idle": "2025-08-14T01:56:34.086972Z",
     "shell.execute_reply": "2025-08-14T01:56:34.083897Z"
    },
    "papermill": {
     "duration": 1.122723,
     "end_time": "2025-08-14T01:56:34.093973",
     "exception": false,
     "start_time": "2025-08-14T01:56:32.971250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['date'], df['sales_sum'])\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('sales_sum')\n",
    "plt.title('Sales Sum over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47cd4b-29e0-4a79-a638-8fecf696997c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:34.194671Z",
     "iopub.status.busy": "2025-08-14T01:56:34.193674Z",
     "iopub.status.idle": "2025-08-14T01:56:34.821536Z",
     "shell.execute_reply": "2025-08-14T01:56:34.817462Z"
    },
    "papermill": {
     "duration": 0.683591,
     "end_time": "2025-08-14T01:56:34.826528",
     "exception": false,
     "start_time": "2025-08-14T01:56:34.142937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the average sales_sum for each day of the week\n",
    "average_sales_by_day = df.groupby('day')['sales_sum'].mean().reindex(['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'])\n",
    "\n",
    "# Plot the linear graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(average_sales_by_day.index, average_sales_by_day.values, marker='o', linestyle='-')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Sales Sum')\n",
    "plt.ylim(0)\n",
    "plt.title('Average Sales Sum by Day of the Week')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49c425-3ede-4859-80ce-2d525734156a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:35.067291Z",
     "iopub.status.busy": "2025-08-14T01:56:35.065364Z",
     "iopub.status.idle": "2025-08-14T01:56:36.757409Z",
     "shell.execute_reply": "2025-08-14T01:56:36.754812Z"
    },
    "papermill": {
     "duration": 1.761758,
     "end_time": "2025-08-14T01:56:36.763470",
     "exception": false,
     "start_time": "2025-08-14T01:56:35.001712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select only the numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "# Create a single figure and axes for the box plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "# Plot box plots for each numeric column\n",
    "df[numeric_cols].boxplot(vert=False, patch_artist=True, showfliers=True) # vert=False for horizontal box plots\n",
    "plt.title('Box Plot of Numeric Variables')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Numeric Variables')\n",
    "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7faa4c-a0b3-4298-bf55-d176fcc1509e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:36.988744Z",
     "iopub.status.busy": "2025-08-14T01:56:36.987786Z",
     "iopub.status.idle": "2025-08-14T01:56:41.606691Z",
     "shell.execute_reply": "2025-08-14T01:56:41.603667Z"
    },
    "papermill": {
     "duration": 4.708638,
     "end_time": "2025-08-14T01:56:41.641711",
     "exception": false,
     "start_time": "2025-08-14T01:56:36.933073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract day of the week and year\n",
    "df['dayofweek'] = df['new_datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['year'] = df['new_datetime'].dt.year\n",
    "\n",
    "# Filter out Saturday (dayofweek == 5)\n",
    "df_filtered_days = df[df['dayofweek'] != 5].copy()\n",
    "\n",
    "# Map dayofweek to English day names\n",
    "day_map = {\n",
    "    0: 'Mon',\n",
    "    1: 'Tue',\n",
    "    2: 'Wed',\n",
    "    3: 'Thu',\n",
    "    4: 'Fri',\n",
    "    5: 'Sat',\n",
    "    6: 'Sun'\n",
    "}\n",
    "df_filtered_days['day_name'] = df_filtered_days['dayofweek'].map(day_map)\n",
    "\n",
    "# Get the list of days to plot (Sun to Fri)\n",
    "days_to_plot = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri']\n",
    "\n",
    "# Create subplots for each day\n",
    "fig, axes = plt.subplots(nrows=len(days_to_plot), ncols=1, figsize=(12, 3 * len(days_to_plot)))\n",
    "fig.suptitle('Seasonal Plot of Average Daily Sales Sum (Sun - Fri)', y=1.02)\n",
    "\n",
    "for i, day in enumerate(days_to_plot):\n",
    "    # Filter data for the current day\n",
    "    df_day = df_filtered_days[df_filtered_days['day_name'] == day].copy()\n",
    "\n",
    "    # Group by date and calculate the average sales sum for that date\n",
    "    # (even though there's usually only one entry per date, this handles potential duplicates gracefully)\n",
    "    daily_avg_sales = df_day.groupby('new_datetime')['sales_sum'].mean()\n",
    "\n",
    "    # Plot the average daily sales for the current day\n",
    "    axes[i].plot(daily_avg_sales.index, daily_avg_sales.values, label=day)\n",
    "    axes[i].set_title(f'Average Sales Sum on {day}')\n",
    "    axes[i].set_ylabel('Average Sales Sum')\n",
    "    axes[i].grid(True)\n",
    "    axes[i].legend()\n",
    "\n",
    "# Set a common xlabel for all subplots\n",
    "fig.text(0.5, 0.00, 'Date', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be0872-bb6f-4c71-91c9-a4a4135fb592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:42.132438Z",
     "iopub.status.busy": "2025-08-14T01:56:42.131439Z",
     "iopub.status.idle": "2025-08-14T01:56:46.063355Z",
     "shell.execute_reply": "2025-08-14T01:56:46.061335Z"
    },
    "papermill": {
     "duration": 4.064172,
     "end_time": "2025-08-14T01:56:46.071058",
     "exception": false,
     "start_time": "2025-08-14T01:56:42.006886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the 'date' column as the index\n",
    "df_decomp = df.set_index('date')\n",
    "\n",
    "# Select the time series data for decomposition\n",
    "time_series = df_decomp['sales_sum']\n",
    "\n",
    "# Perform additive decomposition\n",
    "decomposition = seasonal_decompose(time_series, model='additive', period=7) # Use period=7 for weekly seasonality\n",
    "\n",
    "# Plot the decomposed components\n",
    "fig = decomposition.plot()\n",
    "fig.set_size_inches(10, 8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae70c8-f683-41fa-9b2b-4a611fcd29dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:46.406577Z",
     "iopub.status.busy": "2025-08-14T01:56:46.405578Z",
     "iopub.status.idle": "2025-08-14T01:56:50.674965Z",
     "shell.execute_reply": "2025-08-14T01:56:50.671942Z"
    },
    "papermill": {
     "duration": 4.358797,
     "end_time": "2025-08-14T01:56:50.679596",
     "exception": false,
     "start_time": "2025-08-14T01:56:46.320799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# เพื่อแยกตามเดือนตามปฏิทินจริงๆ เราสามารถ Resample ข้อมูลเป็นรายเดือนก่อน แล้วจึงทำ Decomposition\n",
    "# Resample to monthly frequency, summing the sales_sum for each month\n",
    "monthly_data = time_series.resample('M').sum()\n",
    "\n",
    "# A better approach for visualizing calendar month patterns is a seasonal plot by month.\n",
    "\n",
    "# We already have a 'new_datetime' column. Let's use that to extract the month.\n",
    "df['month'] = df['new_datetime'].dt.month\n",
    "\n",
    "# Group by month and calculate the average sales sum for each month across all years\n",
    "average_sales_by_month = df.groupby('month')['sales_sum'].mean()\n",
    "\n",
    "# Map month numbers to names for better plotting\n",
    "month_names = {\n",
    "    1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n",
    "    7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'\n",
    "}\n",
    "average_sales_by_month.index = average_sales_by_month.index.map(month_names)\n",
    "\n",
    "# Plot the average sales by calendar month\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(average_sales_by_month.index, average_sales_by_month.values, marker='o', linestyle='-')\n",
    "plt.xlabel('Calendar Month')\n",
    "plt.ylabel('Average Sales Sum')\n",
    "plt.title('Average Sales Sum by Calendar Month Across All Years')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Let's do that as well, as it aligns more with the concept of decomposition plots.\n",
    "# Resample to monthly frequency (sum of sales for each month)\n",
    "monthly_series = time_series.resample('M').sum()\n",
    "\n",
    "# Perform additive decomposition on the monthly aggregated data with period=12\n",
    "# This will show trend and annual seasonality (12-month cycle) within the monthly sums\n",
    "decomposition_monthly_agg = seasonal_decompose(monthly_series, model='additive', period=12)\n",
    "\n",
    "# Plot the decomposed components for monthly aggregated data\n",
    "fig_monthly_agg = decomposition_monthly_agg.plot()\n",
    "fig_monthly_agg.set_size_inches(10, 8)\n",
    "fig_monthly_agg.suptitle('Decomposition of Monthly Sales Sum (Annual Seasonality)', y=1.02) # Add a specific title\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b2634-f4ca-488e-9c4f-c494a0a4181a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:51.050622Z",
     "iopub.status.busy": "2025-08-14T01:56:51.049674Z",
     "iopub.status.idle": "2025-08-14T01:56:51.692596Z",
     "shell.execute_reply": "2025-08-14T01:56:51.690575Z"
    },
    "papermill": {
     "duration": 0.741739,
     "end_time": "2025-08-14T01:56:51.697650",
     "exception": false,
     "start_time": "2025-08-14T01:56:50.955911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by year and month, then sum sales_sum\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "# Create monthly data grouped by year\n",
    "monthly_sales_by_year = df.groupby(['year', 'month'])['sales_sum'].sum().reset_index()\n",
    "\n",
    "# Get the maximum year in the dataset\n",
    "max_year = monthly_sales_by_year['year'].max()\n",
    "\n",
    "# Filter out the last month of the last year\n",
    "monthly_sales_filtered = monthly_sales_by_year[\n",
    "    ~((monthly_sales_by_year['year'] == max_year) & (monthly_sales_by_year['month'] == 12))\n",
    "].copy()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Iterate through each year and plot a line\n",
    "for year in monthly_sales_filtered['year'].unique():\n",
    "    df_year = monthly_sales_filtered[monthly_sales_filtered['year'] == year]\n",
    "\n",
    "    # Ensure all months (1-12) are present for consistent plotting\n",
    "    # Create a full set of months for the year\n",
    "    full_months = pd.DataFrame({'month': range(1, 13)})\n",
    "    df_year_full = pd.merge(full_months, df_year, on='month', how='left')\n",
    "\n",
    "    # Sort by month\n",
    "    df_year_full = df_year_full.sort_values('month')\n",
    "\n",
    "    # Plot the data, excluding the last month if it's the max year and month 12\n",
    "    months_to_plot = range(1, 13)\n",
    "    if year == max_year:\n",
    "        months_to_plot = range(1, 12) # Exclude month 12 for the last year\n",
    "\n",
    "    plt.plot(df_year_full['month'].iloc[months_to_plot.start-1 : months_to_plot.stop-1], # Adjust indexing for months\n",
    "             df_year_full['sales_sum'].iloc[months_to_plot.start-1 : months_to_plot.stop-1],\n",
    "             marker='o', linestyle='-', label=str(year))\n",
    "\n",
    "\n",
    "# Set x-axis labels to month names\n",
    "month_names_short = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "plt.xticks(range(1, 13), month_names_short) # Set ticks for all 12 months\n",
    "\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Sum of Sales')\n",
    "plt.ylim(ymin=0)\n",
    "plt.title('Monthly Sales Sum by Year')\n",
    "plt.legend(title='Year')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735af34-62d6-4638-b690-b4d3b8bc3c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:51.894791Z",
     "iopub.status.busy": "2025-08-14T01:56:51.892834Z",
     "iopub.status.idle": "2025-08-14T01:56:55.919854Z",
     "shell.execute_reply": "2025-08-14T01:56:55.917256Z"
    },
    "papermill": {
     "duration": 4.133724,
     "end_time": "2025-08-14T01:56:55.928903",
     "exception": false,
     "start_time": "2025-08-14T01:56:51.795179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Select only the numeric columns again for correlation calculation\n",
    "numeric_cols_for_corr = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df[numeric_cols_for_corr].corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(\"Correlation Matrix of Numeric Features:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Optional: Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".1f\", linewidths=.5)\n",
    "plt.title('Correlation Heatmap of Numeric Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdbbfe3-fa60-4339-9c79-27d6b020a482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:56.407388Z",
     "iopub.status.busy": "2025-08-14T01:56:56.406394Z",
     "iopub.status.idle": "2025-08-14T01:56:56.704996Z",
     "shell.execute_reply": "2025-08-14T01:56:56.702975Z"
    },
    "papermill": {
     "duration": 0.42726,
     "end_time": "2025-08-14T01:56:56.710992",
     "exception": false,
     "start_time": "2025-08-14T01:56:56.283732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Select numerical features for VIF calculation\n",
    "# Exclude the target variable ('sales_sum') and potentially highly correlated/dependent variables like profit, costs, etc.\n",
    "# Also exclude variables that are essentially identifiers or not meant to be predictors like date, year, month, dayofweek, day_name\n",
    "# Let's select relevant numerical predictors identified from EDA/correlation analysis.\n",
    "# Based on the previous analysis, potential numerical predictors could include:\n",
    "# ismarketday, isschoolday, holiday, isbuddaday, nd_lottery,\n",
    "# sales from other channels (sales_gb, sales_rb) - but remember to use lagged values for prediction!\n",
    "# Weather/pollution variables (T, H, V, PP, pm2.5, o3, no2) - consider multicollinearity\n",
    "\n",
    "# For VIF calculation, we need the predictor variables.\n",
    "# Let's start with a subset of potential numerical predictors excluding the target and cost/profit.\n",
    "# Also, let's be mindful of multicollinearity issues identified (e.g., pm2.5 and pm10).\n",
    "# We'll use a subset of potentially useful features for this example.\n",
    "# Note: VIF is calculated for the independent variables *before* modeling.\n",
    "\n",
    "# First, identify numerical columns that are potential predictors\n",
    "# Exclude 'sales_sum', 'profit', and cost-related columns as they are results/components of sales.\n",
    "# Exclude date-related columns that are not numerical predictors themselves but used for time series analysis.\n",
    "# Exclude 'dayofweek', 'year', 'month' as we might use categorical encoding for time-based features or rely on the time series model itself.\n",
    "numerical_predictors = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Remove the target variable and other dependent/related variables\n",
    "exclude_cols = ['sales_sum', 'profit', 'cost_sum', 'cost_material', 'cost_change',\n",
    "                'date', 'new_datetime', 'year', 'month', 'dayofweek']\n",
    "\n",
    "vif_features = [col for col in numerical_predictors if col not in exclude_cols]\n",
    "\n",
    "# Optional: Further refine vif_features based on EDA/correlation.\n",
    "# For instance, if pm10 is highly correlated with pm2.5 and we decide to use only pm2.5.\n",
    "# Let's remove pm10 for this VIF calculation based on previous observation (corr=0.86 with pm2.5).\n",
    "if 'pm10' in vif_features:\n",
    "    vif_features.remove('pm10')\n",
    "\n",
    "# Create a DataFrame with only the features for VIF calculation\n",
    "X = df[vif_features]\n",
    "\n",
    "# Ensure there are no missing values in the selected columns for VIF calculation\n",
    "X = X.dropna()\n",
    "\n",
    "# Handle potential infinite values that might arise from perfectly correlated variables\n",
    "# A common issue is when one variable is a linear combination of others.\n",
    "# Check for constant columns or columns that are linear combinations.\n",
    "# A simple check is to see if the variance is zero or close to zero.\n",
    "non_constant_cols = X.columns[X.var() > 1e-6] # Remove columns with near-zero variance\n",
    "X = X[non_constant_cols]\n",
    "\n",
    "# If after removing near-zero variance columns, there are still perfect linear combinations,\n",
    "# VIF calculation will fail. This might require domain knowledge or further analysis\n",
    "# to identify and remove one of the perfectly correlated variables.\n",
    "# For example, if 'A', 'B', 'C' are such that A + B = C, then including all three will cause issues.\n",
    "\n",
    "# Calculate VIF for each predictor\n",
    "# Make sure X is not empty\n",
    "if not X.empty:\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X.columns\n",
    "    # calculating VIF for each feature\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "                       for i in range(len(X.columns))]\n",
    "\n",
    "    print(\"\\nVariance Inflation Factor (VIF):\")\n",
    "    print(vif_data)\n",
    "else:\n",
    "    print(\"No suitable numerical features found for VIF calculation after exclusions.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7fc64-462c-4fdf-9ba0-62ec6658e3dd",
   "metadata": {
    "papermill": {
     "duration": 0.109545,
     "end_time": "2025-08-14T01:56:56.936186",
     "exception": false,
     "start_time": "2025-08-14T01:56:56.826641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e58ed-fc7a-4408-84ba-984ed5b12107",
   "metadata": {
    "papermill": {
     "duration": 0.111638,
     "end_time": "2025-08-14T01:56:57.163458",
     "exception": false,
     "start_time": "2025-08-14T01:56:57.051820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b8879-b20e-4624-83ad-34653c9cc054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:57.408813Z",
     "iopub.status.busy": "2025-08-14T01:56:57.407810Z",
     "iopub.status.idle": "2025-08-14T01:56:58.532155Z",
     "shell.execute_reply": "2025-08-14T01:56:58.529134Z"
    },
    "papermill": {
     "duration": 1.257558,
     "end_time": "2025-08-14T01:56:58.537155",
     "exception": false,
     "start_time": "2025-08-14T01:56:57.279597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa187f90-6ec7-4f59-b0e6-bef3b8262f6c",
   "metadata": {
    "papermill": {
     "duration": 0.119787,
     "end_time": "2025-08-14T01:56:58.773553",
     "exception": false,
     "start_time": "2025-08-14T01:56:58.653766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35755a-a670-454f-8891-b915395bb4ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:59.014492Z",
     "iopub.status.busy": "2025-08-14T01:56:59.013482Z",
     "iopub.status.idle": "2025-08-14T01:56:59.104359Z",
     "shell.execute_reply": "2025-08-14T01:56:59.102340Z"
    },
    "papermill": {
     "duration": 0.212617,
     "end_time": "2025-08-14T01:56:59.109405",
     "exception": false,
     "start_time": "2025-08-14T01:56:58.896788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index('date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d5590f-9bac-4d94-94d0-7959dc05b93e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:59.359204Z",
     "iopub.status.busy": "2025-08-14T01:56:59.358202Z",
     "iopub.status.idle": "2025-08-14T01:56:59.373819Z",
     "shell.execute_reply": "2025-08-14T01:56:59.368626Z"
    },
    "papermill": {
     "duration": 0.144249,
     "end_time": "2025-08-14T01:56:59.378766",
     "exception": false,
     "start_time": "2025-08-14T01:56:59.234517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lr = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8ce15-05e6-4ce1-b3b5-082f755e98fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:56:59.620567Z",
     "iopub.status.busy": "2025-08-14T01:56:59.619569Z",
     "iopub.status.idle": "2025-08-14T01:56:59.689082Z",
     "shell.execute_reply": "2025-08-14T01:56:59.685689Z"
    },
    "papermill": {
     "duration": 0.195141,
     "end_time": "2025-08-14T01:56:59.694083",
     "exception": false,
     "start_time": "2025-08-14T01:56:59.498942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lr['is_weekend'] = (df_lr.index.dayofweek == 6).astype(int)\n",
    "df_lr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b29506-6097-4ca2-8b35-7c225b4b9ab6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:00.205884Z",
     "iopub.status.busy": "2025-08-14T01:57:00.202884Z",
     "iopub.status.idle": "2025-08-14T01:57:00.283296Z",
     "shell.execute_reply": "2025-08-14T01:57:00.281013Z"
    },
    "papermill": {
     "duration": 0.213553,
     "end_time": "2025-08-14T01:57:00.288349",
     "exception": false,
     "start_time": "2025-08-14T01:57:00.074796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lr['is_Sun'] = (df_lr.index.dayofweek == 6).astype(int)\n",
    "df_lr['is_Mon'] = (df_lr.index.dayofweek == 0).astype(int)\n",
    "df_lr['is_Tue'] = (df_lr.index.dayofweek == 1).astype(int)\n",
    "df_lr['is_Wed'] = (df_lr.index.dayofweek == 2).astype(int)\n",
    "df_lr['is_Thu'] = (df_lr.index.dayofweek == 3).astype(int)\n",
    "df_lr['is_Fri'] = (df_lr.index.dayofweek == 4).astype(int)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a1286-5eee-45ef-8d88-706e9a668b63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:00.537021Z",
     "iopub.status.busy": "2025-08-14T01:57:00.536020Z",
     "iopub.status.idle": "2025-08-14T01:57:00.604245Z",
     "shell.execute_reply": "2025-08-14T01:57:00.602153Z"
    },
    "papermill": {
     "duration": 0.19286,
     "end_time": "2025-08-14T01:57:00.607250",
     "exception": false,
     "start_time": "2025-08-14T01:57:00.414390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lr['is_Jan'] = (df_lr.index.month == 1).astype(int)\n",
    "df_lr['is_Feb'] = (df_lr.index.month == 2).astype(int)\n",
    "df_lr['is_Mar'] = (df_lr.index.month == 3).astype(int)\n",
    "df_lr['is_Apr'] = (df_lr.index.month == 4).astype(int)\n",
    "df_lr['is_May'] = (df_lr.index.month == 5).astype(int)\n",
    "df_lr['is_Jun'] = (df_lr.index.month == 6).astype(int)\n",
    "df_lr['is_Jul'] = (df_lr.index.month == 7).astype(int)\n",
    "df_lr['is_Aug'] = (df_lr.index.month == 8).astype(int)\n",
    "df_lr['is_Sep'] = (df_lr.index.month == 9).astype(int)\n",
    "df_lr['is_Oct'] = (df_lr.index.month == 10).astype(int)\n",
    "df_lr['is_Nov'] = (df_lr.index.month == 11).astype(int)\n",
    "df_lr['is_Dec'] = (df_lr.index.month == 12).astype(int)\n",
    "df_lr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e9e70-ee1c-4c15-af13-9ab73694fe75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:00.821118Z",
     "iopub.status.busy": "2025-08-14T01:57:00.820119Z",
     "iopub.status.idle": "2025-08-14T01:57:00.886821Z",
     "shell.execute_reply": "2025-08-14T01:57:00.883751Z"
    },
    "papermill": {
     "duration": 0.193234,
     "end_time": "2025-08-14T01:57:00.891539",
     "exception": false,
     "start_time": "2025-08-14T01:57:00.698305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lr.dropna(inplace=True)\n",
    "df_lr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c328d61-9de1-40a7-a2bb-8b034893486c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:01.146572Z",
     "iopub.status.busy": "2025-08-14T01:57:01.145582Z",
     "iopub.status.idle": "2025-08-14T01:57:01.209978Z",
     "shell.execute_reply": "2025-08-14T01:57:01.207139Z"
    },
    "papermill": {
     "duration": 0.20046,
     "end_time": "2025-08-14T01:57:01.214170",
     "exception": false,
     "start_time": "2025-08-14T01:57:01.013710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# คำนวณ time_index จาก index ของ DataFrame\n",
    "df_lr['time_index'] = (df_lr.index - df_lr.index.min()).days\n",
    "\n",
    "df_lr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cf1e0-2dec-41cd-af47-a92db8503f78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:01.466686Z",
     "iopub.status.busy": "2025-08-14T01:57:01.465689Z",
     "iopub.status.idle": "2025-08-14T01:57:01.520296Z",
     "shell.execute_reply": "2025-08-14T01:57:01.517286Z"
    },
    "papermill": {
     "duration": 0.188425,
     "end_time": "2025-08-14T01:57:01.524296",
     "exception": false,
     "start_time": "2025-08-14T01:57:01.335871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lr['lag_1'] = df_lr['sales_sum'].shift(1)\n",
    "df_lr['lag_2'] = df_lr['sales_sum'].shift(2)\n",
    "df_lr['lag_3'] = df_lr['sales_sum'].shift(3)\n",
    "df_lr['lag_4'] = df_lr['sales_sum'].shift(4)\n",
    "df_lr['lag_5'] = df_lr['sales_sum'].shift(5)\n",
    "df_lr['lag_6'] = df_lr['sales_sum'].shift(6)\n",
    "df_lr['lag_7'] = df_lr['sales_sum'].shift(7)\n",
    "df_lr = df_lr.reindex(columns=['sales_sum', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7'\n",
    "                         , 'year', 'time_index'\n",
    "                         #, 'is_Jan', 'is_Feb', 'is_Mar', 'is_Apr', 'is_May', 'is_Jun', 'is_Jul', 'is_Aug', 'is_Sep', 'is_Oct', 'is_Nov', 'is_Dec'\n",
    "                         #, 'ismarketday', 'isschoolday', 'holiday'\n",
    "                         ])\n",
    "\n",
    "df_lr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab171d76-5314-44f6-8768-fbfc5a8f81af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:01.771001Z",
     "iopub.status.busy": "2025-08-14T01:57:01.770001Z",
     "iopub.status.idle": "2025-08-14T01:57:01.784614Z",
     "shell.execute_reply": "2025-08-14T01:57:01.782521Z"
    },
    "papermill": {
     "duration": 0.13843,
     "end_time": "2025-08-14T01:57:01.787616",
     "exception": false,
     "start_time": "2025-08-14T01:57:01.649186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_recursive = [\n",
    "        'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7',\n",
    "        'year', 'time_index'\n",
    "        #, 'is_Jan', 'is_Feb', 'is_Mar', 'is_Apr', 'is_May', 'is_Jun', 'is_Jul', 'is_Aug', 'is_Sep', 'is_Oct', 'is_Nov', 'is_Dec'\n",
    "        #, 'ismarketday', 'isschoolday', 'holiday'\n",
    "    ]\n",
    "\n",
    "df_lr = df_lr[features_recursive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108abcd-d28d-4bc5-87b2-6f3f6e0d5d59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:01.974805Z",
     "iopub.status.busy": "2025-08-14T01:57:01.974805Z",
     "iopub.status.idle": "2025-08-14T01:57:01.987470Z",
     "shell.execute_reply": "2025-08-14T01:57:01.985373Z"
    },
    "papermill": {
     "duration": 0.121652,
     "end_time": "2025-08-14T01:57:01.990389",
     "exception": false,
     "start_time": "2025-08-14T01:57:01.868737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2691564-db56-44c5-9ff7-b121c5219aaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:02.244368Z",
     "iopub.status.busy": "2025-08-14T01:57:02.243358Z",
     "iopub.status.idle": "2025-08-14T01:57:02.330126Z",
     "shell.execute_reply": "2025-08-14T01:57:02.326835Z"
    },
    "papermill": {
     "duration": 0.224413,
     "end_time": "2025-08-14T01:57:02.335152",
     "exception": false,
     "start_time": "2025-08-14T01:57:02.110739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = pickle.load(open('Linear_Regression_model.pkl', 'rb'))\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Get the last 7 lags from the original dataframe for prediction\n",
    "last_7_lags = df_lr[['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7']].tail(1).values.flatten()\n",
    "\n",
    "# Prepare future dates and calendar features for the next 7 days\n",
    "last_date = df_lr.index[-1]\n",
    "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=7, freq='D')\n",
    "future_df_predict = pd.DataFrame(index=future_dates)\n",
    "future_df_predict['year'] = future_df_predict.index.year\n",
    "future_df_predict['time_index'] = range(df_lr['time_index'].max() + 1, df_lr['time_index'].max() + 1 + 7)\n",
    "\n",
    "# Make recursive predictions\n",
    "predictions_recursive_loaded = []\n",
    "current_lags_loaded = last_7_lags.copy()\n",
    "\n",
    "print(\"Starting 7-step recursive forecast using loaded model...\")\n",
    "for i in range(7):\n",
    "    calendar_features_loaded = future_df_predict.iloc[i][['year', 'time_index']]\n",
    "    input_vector_loaded = np.concatenate([current_lags_loaded, calendar_features_loaded.values]).reshape(1, -1)\n",
    "\n",
    "    prediction_loaded = loaded_model.predict(input_vector_loaded)[0]\n",
    "    predictions_recursive_loaded.append(prediction_loaded)\n",
    "\n",
    "    current_lags_loaded = np.roll(current_lags_loaded, -1)\n",
    "    current_lags_loaded[-1] = prediction_loaded\n",
    "    print(f\"Step {i+1}/7 predicted: {prediction_loaded:.2f}\")\n",
    "\n",
    "print(\"\\nRecursive forecasting completed using loaded model.\")\n",
    "\n",
    "# Display the predictions with dates\n",
    "predictions_df = pd.DataFrame({'Date': future_dates, 'Predicted Sales': predictions_recursive_loaded})\n",
    "predictions_df = predictions_df.set_index('Date')\n",
    "print(\"\\n7-Day Sales Predictions:\")\n",
    "display(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb702c3-f143-4ed1-a81d-6f942ea9dfb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:02.577438Z",
     "iopub.status.busy": "2025-08-14T01:57:02.576437Z",
     "iopub.status.idle": "2025-08-14T01:57:03.154116Z",
     "shell.execute_reply": "2025-08-14T01:57:03.151609Z"
    },
    "papermill": {
     "duration": 0.719004,
     "end_time": "2025-08-14T01:57:03.159203",
     "exception": false,
     "start_time": "2025-08-14T01:57:02.440199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure df_original is loaded (assuming it's available from previous cells)\n",
    "# If not, you might need to load it again\n",
    "# df_original = pd.read_csv(\"/content/drive/MyDrive/swuds/t5_thesis/98_progress/dataset/nd_sales_LinearRegression_base.csv\")\n",
    "# df_original['date'] = pd.to_datetime(df_original['date'])\n",
    "# df_original = df_original.set_index('date')\n",
    "\n",
    "# Calculate the average sales_sum for each day of the week for the last 3 months\n",
    "last_date_original = df.index.max()\n",
    "three_months_ago = last_date_original - pd.DateOffset(months=3)\n",
    "recent_df = df[df.index >= three_months_ago].copy() # Use copy to avoid SettingWithCopyWarning\n",
    "\n",
    "average_sales_last_3_months = recent_df.groupby('day')['sales_sum'].mean().reindex(['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'])\n",
    "\n",
    "# Ensure predictions_df is available (assuming it's available from previous cells)\n",
    "# If not, you might need to regenerate predictions or load from a file\n",
    "# predictions_df = ... # Load or generate your predictions_df here\n",
    "\n",
    "# Plot the linear graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(average_sales_last_3_months.index, average_sales_last_3_months.values, marker='o', linestyle='-', color='blue', label='Average Sales Last 3 Months')\n",
    "\n",
    "# Add the predicted sales to the plot\n",
    "# Extract day of the week from predicted dates\n",
    "predicted_days_of_week = predictions_df.index.day_name()\n",
    "plt.plot(predicted_days_of_week, predictions_df['Predicted Sales'].values, marker='x', linestyle='-', color='red', label='Predicted Sales (Next 7 Days)')\n",
    "\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Sales Sum')\n",
    "plt.ylim(0)\n",
    "plt.title('Sales Sum by Day of the Week (Average Last 3 Months vs Predicted)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a18ca-9bd5-45c5-b7ba-905d7cd164fa",
   "metadata": {
    "papermill": {
     "duration": 0.129154,
     "end_time": "2025-08-14T01:57:03.424627",
     "exception": false,
     "start_time": "2025-08-14T01:57:03.295473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11865030-4a92-46f1-b26f-d3ab75533816",
   "metadata": {
    "papermill": {
     "duration": 0.126774,
     "end_time": "2025-08-14T01:57:03.680219",
     "exception": false,
     "start_time": "2025-08-14T01:57:03.553445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b5d8d-d29e-4348-9812-6935e7d13e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:03.937275Z",
     "iopub.status.busy": "2025-08-14T01:57:03.936272Z",
     "iopub.status.idle": "2025-08-14T01:57:03.949115Z",
     "shell.execute_reply": "2025-08-14T01:57:03.947273Z"
    },
    "papermill": {
     "duration": 0.140636,
     "end_time": "2025-08-14T01:57:03.952187",
     "exception": false,
     "start_time": "2025-08-14T01:57:03.811551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964ec06-4e87-4c21-86ce-212cbd84deb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:04.152030Z",
     "iopub.status.busy": "2025-08-14T01:57:04.151026Z",
     "iopub.status.idle": "2025-08-14T01:57:04.298718Z",
     "shell.execute_reply": "2025-08-14T01:57:04.296698Z"
    },
    "papermill": {
     "duration": 0.267676,
     "end_time": "2025-08-14T01:57:04.301718",
     "exception": false,
     "start_time": "2025-08-14T01:57:04.034042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5d12f-ea9d-412e-a015-e004233ea9db",
   "metadata": {
    "papermill": {
     "duration": 0.13181,
     "end_time": "2025-08-14T01:57:04.549435",
     "exception": false,
     "start_time": "2025-08-14T01:57:04.417625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea0768-4c9c-4fda-a6b7-f293376aab81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:04.806881Z",
     "iopub.status.busy": "2025-08-14T01:57:04.806881Z",
     "iopub.status.idle": "2025-08-14T01:57:04.820132Z",
     "shell.execute_reply": "2025-08-14T01:57:04.818119Z"
    },
    "papermill": {
     "duration": 0.147977,
     "end_time": "2025-08-14T01:57:04.823135",
     "exception": false,
     "start_time": "2025-08-14T01:57:04.675158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pp = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a062305-0da5-4563-8d5b-1dbf00f08c42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:05.010213Z",
     "iopub.status.busy": "2025-08-14T01:57:05.009214Z",
     "iopub.status.idle": "2025-08-14T01:57:05.079423Z",
     "shell.execute_reply": "2025-08-14T01:57:05.075324Z"
    },
    "papermill": {
     "duration": 0.179872,
     "end_time": "2025-08-14T01:57:05.084535",
     "exception": false,
     "start_time": "2025-08-14T01:57:04.904663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# รีเซ็ต index เพื่อให้ 'date' กลับมาเป็นคอลัมน์\n",
    "df_pp = df_pp.reset_index()\n",
    "\n",
    "# เปลี่ยนชื่อคอลัมน์สำหรับ Prophet\n",
    "df_pp = df_pp.rename(columns={'date': 'ds', 'sales_sum': 'y'})\n",
    "\n",
    "# แสดงผล 5 แถวแรกเพื่อตรวจสอบ\n",
    "df_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d034f-8544-4192-b66a-425d13351538",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:05.364599Z",
     "iopub.status.busy": "2025-08-14T01:57:05.363592Z",
     "iopub.status.idle": "2025-08-14T01:57:05.382674Z",
     "shell.execute_reply": "2025-08-14T01:57:05.379600Z"
    },
    "papermill": {
     "duration": 0.167898,
     "end_time": "2025-08-14T01:57:05.386686",
     "exception": false,
     "start_time": "2025-08-14T01:57:05.218788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# กำหนดลิสต์ของ Regressor ที่จะใช้\n",
    "regressor_columns = ['ismarketday', 'isschoolday', 'holiday']\n",
    "\n",
    "# เลือกคอลัมน์ที่จำเป็นทั้งหมด\n",
    "df_prophet = df_pp[['ds', 'y'] + regressor_columns].copy()\n",
    "print(\"Data prepared for Prophet with regressors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71915fd3-261b-42bf-82f4-d00a036ae0c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:05.661087Z",
     "iopub.status.busy": "2025-08-14T01:57:05.660083Z",
     "iopub.status.idle": "2025-08-14T01:57:05.675726Z",
     "shell.execute_reply": "2025-08-14T01:57:05.673704Z"
    },
    "papermill": {
     "duration": 0.154917,
     "end_time": "2025-08-14T01:57:05.678797",
     "exception": false,
     "start_time": "2025-08-14T01:57:05.523880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657a6d9-2ac3-4889-8dd6-fa381bfa2659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:05.955184Z",
     "iopub.status.busy": "2025-08-14T01:57:05.955184Z",
     "iopub.status.idle": "2025-08-14T01:57:07.074312Z",
     "shell.execute_reply": "2025-08-14T01:57:07.071289Z"
    },
    "papermill": {
     "duration": 1.270044,
     "end_time": "2025-08-14T01:57:07.079399",
     "exception": false,
     "start_time": "2025-08-14T01:57:05.809355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "\n",
    "# Load the model\n",
    "with open('Prophet_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "print(\"Prophet model loaded successfully.\")\n",
    "\n",
    "# Create a future dataframe for the next 7 days\n",
    "# Find the last date in your original dataframe (df_prophet)\n",
    "last_date = df_prophet['ds'].max()\n",
    "\n",
    "# Generate future dates starting from the day after the last date\n",
    "future_dates = loaded_model.make_future_dataframe(periods=9, freq='D', include_history=False) # Generate a bit more to ensure 7 non-Saturdays\n",
    "\n",
    "# Filter out Saturdays\n",
    "future_no_saturday = future_dates[future_dates['ds'].dt.dayofweek != 5]\n",
    "\n",
    "# Take the first 7 future dates that are not Saturdays\n",
    "future_7_days = future_no_saturday.head(7)\n",
    "\n",
    "# Merge with the regressors from the original data for these future dates\n",
    "# If you have future regressor values, you should load or generate them here.\n",
    "# For demonstration, we will use the ffill method as in previous steps, assuming\n",
    "# the regressor values from the last known date are carried forward.\n",
    "future_with_regressors = pd.merge(future_7_days, df_prophet[['ds'] + regressor_columns], on='ds', how='left')\n",
    "\n",
    "# Fill any missing regressor values (e.g., for dates beyond the original data)\n",
    "future_with_regressors.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "print(\"\\nFuture dates for forecasting (next 7 non-Saturdays):\")\n",
    "print(future_with_regressors)\n",
    "\n",
    "# Make predictions\n",
    "future_forecast = loaded_model.predict(future_with_regressors)\n",
    "\n",
    "print(\"\\nForecast for the next 7 days:\")\n",
    "print(future_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f49906-f927-4e1d-8c5a-7b518f56606f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T01:57:07.337806Z",
     "iopub.status.busy": "2025-08-14T01:57:07.336808Z",
     "iopub.status.idle": "2025-08-14T01:57:07.987037Z",
     "shell.execute_reply": "2025-08-14T01:57:07.984035Z"
    },
    "papermill": {
     "duration": 0.780305,
     "end_time": "2025-08-14T01:57:07.991052",
     "exception": false,
     "start_time": "2025-08-14T01:57:07.210747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Calculate the day of the week for each date in df_prophet\n",
    "df_prophet['dayofweek'] = df_prophet['ds'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# 2. Calculate the 3-month rolling average for each day of the week\n",
    "# We need to calculate this for each day of the week separately to get the average for each specific day\n",
    "# Create a temporary DataFrame for calculating rolling average\n",
    "df_temp = df_prophet.copy()\n",
    "df_temp['rolling_avg_3m'] = df_temp.groupby('dayofweek')['y'].transform(lambda x: x.rolling(window=12, min_periods=1).mean()) # Assuming roughly 4 weeks/month * 3 months = 12 data points\n",
    "\n",
    "# Change this line to use .last()\n",
    "# This creates a DataFrame with the last entry for each day of the week\n",
    "average_sales_last_3_months = df_temp.groupby('dayofweek').last()\n",
    "\n",
    "# The rest of your code from this point on will work correctly\n",
    "# For example, when you reset the index, 'dayofweek' will become a column\n",
    "average_sales_last_3_months_plot = average_sales_last_3_months.reset_index()\n",
    "\n",
    "# This line will now succeed\n",
    "average_sales_last_3_months_plot['day'] = average_sales_last_3_months_plot['dayofweek'].map(day_map)\n",
    "\n",
    "# Sort by day of the week\n",
    "average_sales_last_3_months = average_sales_last_3_months.sort_index()\n",
    "\n",
    "# 4. Prepare the forecast data for plotting (last 7 days forecast)\n",
    "# We already have 'future_forecast' from the previous step which contains the next 7 days forecast\n",
    "# Ensure the forecast data is also sorted by day of the week for consistent plotting\n",
    "future_forecast['dayofweek'] = future_forecast['ds'].dt.dayofweek\n",
    "forecast_for_plot = future_forecast.sort_values(by='dayofweek')\n",
    "\n",
    "# Ensure we only have one forecast point per day of the week if there are duplicates\n",
    "forecast_for_plot = forecast_for_plot.drop_duplicates(subset=['dayofweek'], keep='first')\n",
    "\n",
    "\n",
    "# 5. Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define the order of days of the week for plotting\n",
    "day_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sun']\n",
    "day_map = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 6: 'Sun'}\n",
    "\n",
    "# Plot Average Sales (last 3 months)\n",
    "# Map the dayofweek index to the string representation\n",
    "average_sales_last_3_months_plot = average_sales_last_3_months.reset_index()\n",
    "average_sales_last_3_months_plot['day'] = average_sales_last_3_months_plot['dayofweek'].map(day_map)\n",
    "average_sales_last_3_months_plot = average_sales_last_3_months_plot.set_index('day').loc[day_order]\n",
    "\n",
    "# Ensure that all days in day_order are present in average_sales_last_3_months_plot before plotting\n",
    "# If a day is missing, its value will be NaN, which matplotlib handles correctly for line plots\n",
    "plt.plot(average_sales_last_3_months_plot.index, average_sales_last_3_months_plot['rolling_avg_3m'], marker='o', linestyle='-', color='blue', label='Average Sales (Last 3 Months)')\n",
    "\n",
    "# Plot Prophet Forecast\n",
    "forecast_for_plot['day'] = forecast_for_plot['dayofweek'].map(day_map)\n",
    "forecast_for_plot = forecast_for_plot.set_index('day').loc[day_order]\n",
    "\n",
    "# Ensure that all days in day_order are present in forecast_for_plot before plotting\n",
    "plt.plot(forecast_for_plot.index, forecast_for_plot['yhat'], marker='x', linestyle='-', color='red', label='Prophet Forecast')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Average Sales (Last 3 Months) vs. Prophet Forecast by Day of the Week')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.912286,
   "end_time": "2025-08-14T01:57:09.655372",
   "environment_variables": {},
   "exception": null,
   "input_path": "C:\\Users\\pao5_\\Pao_Project\\nd_1click\\01_sales\\00_EDA_Linear_Prophet.ipynb",
   "output_path": "C:\\Users\\pao5_\\Pao_Project\\nd_1click\\01_sales\\00_EDA_Linear_Prophet.ipynb",
   "parameters": {},
   "start_time": "2025-08-14T01:56:26.743086",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
